{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8810052,"sourceType":"datasetVersion","datasetId":5299098},{"sourceId":8811729,"sourceType":"datasetVersion","datasetId":5300360}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-07-01T04:22:54.451909Z","iopub.execute_input":"2024-07-01T04:22:54.452244Z","iopub.status.idle":"2024-07-01T04:23:08.183538Z","shell.execute_reply.started":"2024-07-01T04:22:54.452217Z","shell.execute_reply":"2024-07-01T04:23:08.182614Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pickle\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport transformers\nfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig, AutoModel,BitsAndBytesConfig,GenerationConfig\nfrom safetensors.torch import load_file\nfrom datasets import load_dataset\nfrom typing import Union\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    set_peft_model_state_dict,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T04:23:08.185556Z","iopub.execute_input":"2024-07-01T04:23:08.185865Z","iopub.status.idle":"2024-07-01T04:23:15.656879Z","shell.execute_reply.started":"2024-07-01T04:23:08.185817Z","shell.execute_reply":"2024-07-01T04:23:15.656164Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Recoding Source :**","metadata":{}},{"cell_type":"code","source":"class ODIE():\n    #Initial :\n    def __init__(self, \n                old_output = None,\n                base_model = 'TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T', #Set base model : \n                model_config = 'adapter_config.json',   #Tên file cấu hình mô hình\n                model_weights = 'adapter_model.safetensors',  #Tên file trọng số mô hình\n                target_modules = [\n                        'q_proj', 'k_proj', 'v_proj', 'o_proj', \n                        'up_proj', 'down_proj', 'gate_proj', \n                        'embed_tokens', 'lm_head'\n                ],\n                template_path = '/kaggle/input/data-nlp-2/Data_train/templates/alpaca.json',\n               ):\n        self.old_output = old_output\n        self.base_model = base_model\n        self.model_config = model_config\n        self.model_weights = model_weights\n        self.target_modules = target_modules\n        self.template_path = template_path\n        self.tokenizer = self.Set_Tokenize()\n        self.check_train = True\n        \n    #Function Load Model : \n    def Load_Model(self):\n        print('========================Please Waiting==============================')\n        print('====================================================================')\n        print('========================Loading Model===============================')\n        output_dir = self.old_output\n        #Tải lại cấu hình mô hình :\n        model = AutoModelForCausalLM.from_pretrained(self.base_model, #Base Model\n                                                torch_dtype=torch.float32,\n                                                load_in_8bit = False,\n                                                is_decoder=True\n                                                )\n        \n        if output_dir is not None :\n            print('Reloading and Retraining Model!')\n            print('======================================================')\n            #Đường dẫn tới thư mục chứa các tệp mô hình đã lưu :\n            config_path = os.path.join(output_dir, self.model_config)\n            model_weights_path = os.path.join(output_dir, self.model_weights)\n\n            with open(config_path, 'r') as f:\n                config_dict = json.load(f)\n\n            #Chuyển đổi target_modules thành List thay vì Set :\n            if isinstance(config_dict['target_modules'], list):\n                target_modules = config_dict['target_modules']\n            else:\n                target_modules = list(config_dict['target_modules'])\n\n            config = LoraConfig(**config_dict)\n\n            #Set Inference_Mode là False để tiếp tục train :\n            config.inference_mode = False\n\n            #Set lại target_modules thành List :\n            config.target_modules = target_modules\n\n            model = get_peft_model(model, config)\n\n            #Kiểm tra weights và load weights :\n            checkpoint_name = model_weights_path\n            if os.path.exists(checkpoint_name):\n                print(f\"Restarting from {checkpoint_name}\")\n                print('======================================================')\n                adapters_weights = load_file(checkpoint_name)\n                set_peft_model_state_dict(model, adapters_weights)\n                print('Loading sucessful checkpoint')\n            else:\n                print(f\"Checkpoint {checkpoint_name} not found\")\n\n            print('Reloading Model Complition!')\n        else :\n            print('Training From First!')\n            config = LoraConfig(\n                    r=16,  \n                    lora_alpha=16,\n                    target_modules= self.target_modules,\n                    lora_dropout=0.1,\n                    bias=\"none\",\n                    task_type=\"CAUSAL_LM\"\n                )\n            model = get_peft_model(model, config)\n        return model\n    \n    #Set Tokenizer : \n    def Set_Tokenize(self): \n        #Get Tokenize :\n        tokenizer = AutoTokenizer.from_pretrained(self.base_model,legacy=False)\n\n        #Set padding : \n        tokenizer.pad_token_id = 0\n        \n        #Set location of padding :\n        tokenizer.padding_side = \"left\"  # Allow batched inference\n    \n        return tokenizer\n        \n    #Tokenize Processing : \n    def Tokenize(self,prompt,add_eos_token=True,cutoff_len = 1024):\n        check_train = self.check_train\n#         if check_train :\n#             padding = False \n#             return_tensors = None\n#         else :\n#             return_tensors = 'pt'\n#             padding = 'max_length' \n\n            \n        tokenizer = self.tokenizer\n        result = tokenizer(\n                prompt,\n                truncation=True,\n                max_length=cutoff_len,\n                padding=False,\n                return_tensors=None,\n                )\n        if check_train :\n            if (\n                result[\"input_ids\"][-1] != tokenizer.eos_token_id\n                and len(result[\"input_ids\"]) < cutoff_len\n                and add_eos_token\n            ):\n                result[\"input_ids\"].append(tokenizer.eos_token_id)\n                result[\"attention_mask\"].append(1)\n            result['labels'] = result['input_ids'].copy()\n            \n        return result\n    \n    #Tokenize Prompt :\n    def Tokenize_Prompt(self,data_point):\n        #Prompt Processing Function :\n        def Prompt_Processing(\n            instructions,\n            inputs: Union[None, str] = None,\n            labels: Union[None, str] = None\n        )-> str:\n            #Loading template : \n            with open(self.template_path) as fp:\n                template = json.load(fp)\n\n            #Generate Prompt with inputs : \n            if inputs:\n                res = template[\"prompt_input\"].format(\n                instruction=instructions, input=inputs\n                    )\n            #Generate Prompt no inputs :\n            else:\n                res = template[\"prompt_no_input\"].format(\n                        instruction=instructions\n                    )\n\n            #Concatenate Outputs with Prompt :\n            if labels:\n                res = f\"{res}{labels}\"\n            return res\n        #Process Prompt :\n        if self.check_train:\n            full_prompt = Prompt_Processing(\n                                data_point[\"instruction\"], #Instruction\n                                data_point[\"text\"], #Input \n                                data_point[\"table\"] #Output\n            )\n\n\n            #Tokenize Prompt :\n            tokenized_full_prompt = self.Tokenize(full_prompt,add_eos_token=True)\n\n            #Train on input: \n            add_eos_token = True\n            user_prompt = Prompt_Processing(\n                            data_point[\"instruction\"], data_point[\"text\"]\n                            )\n\n            tokenized_user_prompt = self.Tokenize(\n                        user_prompt, add_eos_token= add_eos_token\n                    )\n            user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n\n            if add_eos_token:\n                user_prompt_len -= 1\n\n            tokenized_full_prompt[\"labels\"] = [\n                        -100\n                    ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n                        user_prompt_len:\n                    ]\n            \n            return tokenized_full_prompt\n        else : \n            data = {}\n            full_prompt = Prompt_Processing(\n                            data_point[\"instruction\"], data_point[\"text\"]\n                        )\n            data['input'] = full_prompt\n            data['output'] = data_point[\"table\"]\n            return data\n    \n    #Loading data :\n    def Data_Processing(self,path_data,start_row = 0, num_rows = 30, val_set_size = 0,check_train = True):\n        #Read File data :\n        if path_data.endswith(\".json\") or path_data.endswith(\".jsonl\"):\n            data = load_dataset(\"json\", data_files=path_data,split=f\"train[{start_row}:{start_row + num_rows}]\")\n        else:\n            data = load_dataset(path_data)\n\n        if 'messages' in data[0]:\n            data = data.remove_columns('messages')\n\n        #Set Check_Train\n        self.check_train = check_train\n        \n        if check_train :\n            #Split data to data train and data validtions : \n            if val_set_size > 0:\n                train_val = data.train_test_split(\n                    test_size=val_set_size, shuffle=True, seed=42\n                )\n                train_data = train_val['train'].shuffle().map(self.Tokenize_Prompt)\n\n                val_data = train_val['test'].shuffle().map(self.Tokenize_Prompt)\n                \n            else:\n                train_data = data.shuffle().map(self.Tokenize_Prompt)\n                val_data = None\n\n            return train_data, val_data\n        else :            \n\n            #Tokenize data :\n            test_data = data.shuffle().map(self.Tokenize_Prompt)\n            \n\n            return test_data\n\n        \n            \n    #Training Model Fucntion :\n    def Training_Model(\n        self,\n        train_data, val_data = None,\n        batch_size = 8,\n        gradient_accumulation_steps = 2,\n        num_epochs = 10,\n        learning_rate = 1e-4,\n        output_dir = 'Model_Output',\n        resume_checkpoint = None,\n        is_trainable = True #Thực hiện cho phép retrain \n    ):\n                \n        #Load Model :\n        model = self.Load_Model()\n        \n        #Get Tokenizer :\n        tokenizer = self.tokenizer\n\n        #Information model :\n        print('================Information Model=====================')\n        print('======================================================')\n\n#         #Remove các đặc trưng không cần thiết : \n#         list_remove = ['instruction','domain', 'text', 'category', 'table']\n#         for i in list_remove :\n#             if i in train_data[0] :\n#                 train_data = train_data.remove_columns(i)\n#                 val_data = val_data.remove_columns(i)\n        \n        \n        #Print information of trainable_paramers :\n        def print_trainable_parameters_custom(model):\n            trainable_params = [p for p in model.parameters() if p.requires_grad]\n            total_params = sum(p.numel() for p in trainable_params)\n            print(f\"Number of trainable parameters: {total_params}\")\n            for name, param in model.named_parameters():\n                if param.requires_grad:\n                    print(f\"{name}: {param.shape}\")\n\n        try:\n            model.print_trainable_parameters()\n        except AttributeError:\n            print_trainable_parameters_custom(model)\n\n        #Set Transformers Arguments : \n        trans_argu = transformers.TrainingArguments(\n                    per_device_train_batch_size=batch_size,\n                    per_device_eval_batch_size=batch_size,\n                    gradient_accumulation_steps=gradient_accumulation_steps,\n                    warmup_ratio=0.03,\n                    num_train_epochs=num_epochs,\n                    learning_rate=learning_rate,\n                    fp16=True, #Change\n                    logging_steps=10,\n                    optim=\"adamw_torch\",\n                    save_strategy=\"epoch\",\n                    eval_strategy=\"epoch\",\n                    output_dir=output_dir,\n                    save_total_limit=2, #Số lượng checkpoints tối đa được lưu\n                    load_best_model_at_end=True,\n                    gradient_checkpointing=True,\n                    weight_decay=0.01                \n        )\n        #Set Data Colacttor : \n        data_collator=transformers.DataCollatorForSeq2Seq(\n                    tokenizer, \n                    pad_to_multiple_of=8, \n                    return_tensors=\"pt\", \n                    padding=True,\n                    label_pad_token_id = -100\n                    )\n        \n\n\n        print('======================================================')\n        #Loss :\n\n\n        #Initialize Custom Trainer with custom loss function :\n        trainer = transformers.Trainer(\n            model=model,\n            args=trans_argu,\n            train_dataset=train_data,\n            eval_dataset=val_data,\n            tokenizer=tokenizer,\n            data_collator=data_collator\n        )\n\n        \n        model.config.use_cache = False\n        #Compile model to optimize : \n        if torch.__version__ >= '2' :\n            print('Compile Model with Torch :')\n            print('======================================================')\n            model = torch.compile(model)\n        \n\n        print('======================Training============================')\n        trainer.train(resume_from_checkpoint=resume_checkpoint)\n        \n        print('========================Saving==============================')\n        model.save_pretrained(os.path.join(output_dir, 'ODIE_Model'), is_trainable = is_trainable)\n        tokenizer.save_pretrained(os.path.join(output_dir, 'ODIE_Model'))\n        \n\n        print('Training Model Completion!')\n        return model, trainer\n    \n    def Get_Response(self, output: str) -> str:\n        #Loading template : \n        with open(self.template_path) as fp:\n            template = json.load(fp)\n        return output.split(template[\"response_split\"])[1].strip()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T04:23:15.658633Z","iopub.execute_input":"2024-07-01T04:23:15.659223Z","iopub.status.idle":"2024-07-01T04:23:15.701454Z","shell.execute_reply.started":"2024-07-01T04:23:15.659190Z","shell.execute_reply":"2024-07-01T04:23:15.700292Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# path_old_output = '/kaggle/input/d/hoangtruongnlp/output-premodel/Model'\nMy_ODIE = ODIE()","metadata":{"execution":{"iopub.status.busy":"2024-07-01T04:23:15.703629Z","iopub.execute_input":"2024-07-01T04:23:15.704168Z","iopub.status.idle":"2024-07-01T04:23:17.001544Z","shell.execute_reply.started":"2024-07-01T04:23:15.704142Z","shell.execute_reply":"2024-07-01T04:23:17.000751Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff78f9356e546158e646fd81720159d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dd05773b0a14698899bffa984f5b1ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ae32ddcf3d4c548cf837934614b24f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"897e7c6ca93446b1bb00084c145741e4"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Get Data**","metadata":{}},{"cell_type":"code","source":"path_data_train = '/kaggle/input/data-nlp-2/Data_train/training_data.jsonl'\npath_data_test = '/kaggle/input/data-nlp-2/Data_train/test.json'\ntrain_data, val_data = My_ODIE.Data_Processing(path_data = path_data_train ,start_row = 2000, num_rows = 4000, val_set_size = 400, check_train = True)\ntest_data = My_ODIE.Data_Processing(path_data = path_data_test, start_row = 20, num_rows = 5, val_set_size = 0, check_train = False)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T04:23:17.002694Z","iopub.execute_input":"2024-07-01T04:23:17.003023Z","iopub.status.idle":"2024-07-01T04:23:39.486183Z","shell.execute_reply.started":"2024-07-01T04:23:17.002998Z","shell.execute_reply":"2024-07-01T04:23:39.485324Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a59b4a30734dac9a1d0b0a0c703198"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa9affa8c4c41ababaf699753037c50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9850c024d0a4d388f546853a1137983"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14987f4d64944139ad418fd9e4076a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b43478a7c4f348c898bd4f484832b270"}},"metadata":{}}]},{"cell_type":"code","source":"print(train_data)\nprint(val_data)\nprint(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T04:23:39.487250Z","iopub.execute_input":"2024-07-01T04:23:39.487513Z","iopub.status.idle":"2024-07-01T04:23:39.492522Z","shell.execute_reply.started":"2024-07-01T04:23:39.487491Z","shell.execute_reply":"2024-07-01T04:23:39.491582Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['instruction', 'domain', 'text', 'category', 'table', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 3600\n})\nDataset({\n    features: ['instruction', 'domain', 'text', 'category', 'table', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 400\n})\nDataset({\n    features: ['table', 'difficulty', 'instruction', 'source', 'category', 'text', 'source_type', 'domain', 'input', 'output'],\n    num_rows: 5\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(1) :\n    print('Dataset Training :')\n    print(train_data['input_ids'][i])\n    print(train_data['attention_mask'][i])    \n    print(train_data['labels'][i])\n    \n    print()\n    print('Dataset Validation :')\n    print('Dataset Training :')\n    print(val_data['input_ids'][i])\n    print(val_data['attention_mask'][i])    \n    print(val_data['labels'][i])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T04:23:39.493742Z","iopub.execute_input":"2024-07-01T04:23:39.494346Z","iopub.status.idle":"2024-07-01T04:23:44.702134Z","shell.execute_reply.started":"2024-07-01T04:23:39.494305Z","shell.execute_reply":"2024-07-01T04:23:44.701166Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Dataset Training :\n[1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 5647, 1461, 278, 8018, 2472, 515, 278, 2183, 1426, 393, 5353, 267, 278, 25486, 322, 7037, 310, 23011, 519, 5864, 8974, 297, 11781, 278, 3186, 29915, 29879, 5864, 4225, 29889, 13, 13, 2277, 29937, 10567, 29901, 13, 29934, 264, 809, 519, 5864, 8974, 526, 263, 19328, 15678, 8569, 297, 16021, 5925, 29889, 9267, 5925, 15055, 505, 3902, 4395, 278, 1900, 3519, 363, 4023, 2264, 292, 23011, 519, 5864, 322, 967, 23633, 363, 1716, 278, 5177, 322, 12459, 29889, 512, 697, 6559, 29892, 5925, 414, 4392, 1312, 278, 28326, 4127, 310, 773, 4768, 290, 465, 304, 5706, 5864, 29889, 2688, 1476, 393, 773, 4768, 290, 465, 408, 263, 7601, 5864, 2752, 756, 1784, 29380, 322, 17407, 25486, 975, 13807, 21983, 309, 4084, 1379, 29889, 7280, 6559, 21103, 3598, 19030, 278, 7037, 310, 21635, 5864, 297, 263, 1353, 310, 1422, 23136, 29889, 450, 5925, 10943, 393, 29892, 15020, 777, 4216, 1627, 29879, 322, 21833, 297, 5864, 1962, 29892, 21635, 3081, 338, 263, 10712, 10631, 681, 322, 8716, 519, 1650, 363, 11781, 278, 3186, 29915, 29879, 5864, 4225, 29889, 7280, 5925, 5650, 628, 1490, 964, 278, 23633, 310, 8805, 3081, 363, 1716, 20201, 616, 322, 12128, 671, 29889, 450, 6559, 28585, 393, 8805, 5864, 338, 263, 10712, 11828, 322, 15075, 475, 519, 1650, 363, 27668, 21180, 918, 322, 7933, 8697, 10489, 953, 6847, 29889, 6811, 497, 29892, 1438, 11898, 12141, 278, 7282, 7037, 310, 23011, 519, 5864, 8974, 304, 3867, 15075, 475, 519, 322, 3438, 29899, 15987, 573, 5864, 6851, 363, 1749, 3186, 29889, 13, 13, 2277, 29937, 13291, 29901, 13, 29989, 7493, 809, 519, 24836, 7562, 891, 25215, 19771, 891, 10173, 2556, 891, 13, 29989, 11474, 891, 11474, 891, 11474, 891, 13, 29989, 3457, 290, 465, 891, 16738, 284, 322, 17407, 25486, 975, 13807, 21983, 309, 4084, 1379, 891, 5169, 294, 1821, 7601, 5864, 2752, 891, 13, 29989, 4956, 279, 24836, 891, 5057, 368, 10631, 681, 322, 8716, 519, 1650, 363, 11781, 278, 3186, 29915, 29879, 5864, 4225, 891, 7338, 575, 3598, 19030, 7037, 297, 5164, 23136, 891, 13, 29989, 17311, 9206, 891, 5057, 368, 11828, 322, 15075, 475, 519, 1650, 363, 27668, 21180, 918, 322, 7933, 8697, 10489, 953, 6847, 891, 4111, 1389, 1169, 363, 1716, 20201, 616, 322, 12128, 671, 891, 2]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29989, 7493, 809, 519, 24836, 7562, 891, 25215, 19771, 891, 10173, 2556, 891, 13, 29989, 11474, 891, 11474, 891, 11474, 891, 13, 29989, 3457, 290, 465, 891, 16738, 284, 322, 17407, 25486, 975, 13807, 21983, 309, 4084, 1379, 891, 5169, 294, 1821, 7601, 5864, 2752, 891, 13, 29989, 4956, 279, 24836, 891, 5057, 368, 10631, 681, 322, 8716, 519, 1650, 363, 11781, 278, 3186, 29915, 29879, 5864, 4225, 891, 7338, 575, 3598, 19030, 7037, 297, 5164, 23136, 891, 13, 29989, 17311, 9206, 891, 5057, 368, 11828, 322, 15075, 475, 519, 1650, 363, 27668, 21180, 918, 322, 7933, 8697, 10489, 953, 6847, 891, 4111, 1389, 1169, 363, 1716, 20201, 616, 322, 12128, 671, 891, 2]\n\nDataset Validation :\nDataset Training :\n[1, 13866, 338, 385, 15278, 393, 16612, 263, 3414, 29892, 3300, 2859, 411, 385, 1881, 393, 8128, 4340, 3030, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 5647, 1461, 278, 8018, 4902, 515, 278, 4944, 1426, 29889, 13, 13, 2277, 29937, 10567, 29901, 13, 797, 14917, 396, 29900, 29900, 29896, 13, 15122, 4408, 29901, 2259, 7075, 13, 7514, 4712, 29901, 5468, 29871, 29906, 299, 29892, 29871, 29906, 29900, 29906, 29941, 13, 7514, 25085, 29901, 13, 29899, 323, 29899, 845, 2728, 313, 3306, 29901, 6054, 29892, 21179, 29901, 3436, 1974, 29897, 448, 395, 29906, 29900, 29889, 29929, 29929, 13, 29899, 2581, 550, 313, 3306, 29901, 10924, 29892, 21179, 29901, 29871, 29941, 29946, 29897, 448, 395, 29941, 29929, 29889, 29929, 29929, 13, 29899, 317, 686, 605, 267, 313, 29933, 9502, 29901, 9596, 29899, 29933, 273, 29897, 448, 395, 29896, 29900, 29929, 29889, 29929, 29929, 13, 29933, 8873, 10343, 29901, 13, 7061, 29901, 29871, 29896, 29906, 29941, 4241, 7103, 29892, 3139, 27734, 29892, 8278, 13, 9861, 29901, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29945, 13, 11536, 29901, 395, 29896, 29955, 29900, 29889, 29929, 29955, 13, 15467, 358, 21864, 29901, 24596, 277, 9160, 13, 13, 797, 14917, 396, 29900, 29900, 29906, 13, 15122, 4408, 29901, 10447, 1938, 29872, 13, 7514, 4712, 29901, 5468, 29871, 29945, 386, 29892, 29871, 29906, 29900, 29906, 29941, 13, 7514, 25085, 29901, 13, 29899, 360, 1253, 313, 3306, 29901, 4367, 29892, 21179, 29901, 18285, 29897, 448, 395, 29945, 29900, 29889, 29900, 29900, 13, 29899, 382, 2749, 886, 313, 29933, 9502, 29901, 23738, 1706, 1943, 29897, 448, 395, 29946, 29945, 29889, 29900, 29900, 13, 29933, 8873, 10343, 29901, 13, 7061, 29901, 29871, 29946, 29945, 29953, 7315, 280, 18874, 29892, 3139, 12690, 29892, 8278, 13, 9861, 29901, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29953, 13, 11536, 29901, 395, 29929, 29945, 29889, 29900, 29900, 13, 15467, 358, 21864, 29901, 14617, 18210, 13, 13, 797, 14917, 396, 29900, 29900, 29941, 13, 15122, 4408, 29901, 5765, 11717, 13, 7514, 4712, 29901, 5468, 29871, 29929, 386, 29892, 29871, 29906, 29900, 29906, 29941, 13, 7514, 25085, 29901, 13, 29899, 19509, 17550, 267, 313, 29933, 9502, 29901, 405, 9345, 29892, 21179, 29901, 29871, 29896, 29900, 29897, 448, 395, 29896, 29906, 29929, 29889, 29929, 29929, 13, 29899, 323, 29899, 845, 2728, 313, 3306, 29901, 25529, 29892, 21179, 29901, 8218, 479, 29897, 448, 395, 29896, 29929, 29889, 29929, 29929, 13, 29899, 435, 468, 3460, 349, 1934, 313, 29933, 9502, 29901, 2087, 8817, 29892, 21179, 29901, 3436, 1974, 29897, 448, 395, 29946, 29929, 29889, 29929, 29929, 13, 29933, 8873, 10343, 29901, 13, 7061, 29901, 29871, 29955, 29947, 29929, 24132, 7103, 29892, 530, 858, 403, 29892, 8278, 13, 9861, 29901, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29955, 13, 11536, 29901, 395, 29896, 29929, 29929, 29889, 29929, 29955, 13, 15467, 358, 21864, 29901, 7089, 277, 9160, 13, 13, 12148, 4443, 393, 1438, 297, 1365, 1575, 526, 26797, 1848, 322, 363, 8632, 1230, 11976, 871, 29889, 13, 13, 2277, 29937, 13291, 29901, 13, 29989, 512, 14917, 396, 891, 21886, 4408, 891, 8170, 4712, 891, 8170, 25085, 891, 6682, 292, 16428, 891, 24323, 891, 14990, 891, 14617, 358, 21864, 891, 13, 29989, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 13, 29989, 29871, 29900, 29900, 29896, 891, 2259, 7075, 891, 5468, 29871, 29906, 299, 29892, 29871, 29906, 29900, 29906, 29941, 891, 323, 29899, 845, 2728, 313, 18700, 29892, 3436, 1974, 511, 2581, 550, 313, 21319, 29892, 29871, 29941, 29946, 511, 317, 686, 605, 267, 313, 29934, 388, 29899, 29933, 273, 29897, 891, 29871, 29896, 29906, 29941, 4241, 7103, 29892, 3139, 27734, 29892, 8278, 891, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29945, 891, 395, 29896, 29955, 29900, 29889, 29929, 29955, 891, 24596, 277, 9160, 891, 13, 29989, 29871, 29900, 29900, 29906, 891, 10447, 1938, 29872, 891, 5468, 29871, 29945, 386, 29892, 29871, 29906, 29900, 29906, 29941, 891, 360, 1253, 313, 9039, 29892, 18285, 511, 382, 2749, 886, 313, 29968, 403, 1706, 1943, 29897, 891, 29871, 29946, 29945, 29953, 7315, 280, 18874, 29892, 3139, 12690, 29892, 8278, 891, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29953, 891, 395, 29929, 29945, 29889, 29900, 29900, 891, 14617, 18210, 891, 13, 29989, 29871, 29900, 29900, 29941, 891, 5765, 11717, 891, 5468, 29871, 29929, 386, 29892, 29871, 29906, 29900, 29906, 29941, 891, 19509, 17550, 267, 313, 29940, 9345, 29892, 29871, 29896, 29900, 511, 323, 29899, 845, 2728, 313, 29954, 8903, 29892, 8218, 479, 511, 435, 468, 3460, 349, 1934, 313, 3253, 8817, 29892, 3436, 1974, 29897, 891, 29871, 29955, 29947, 29929, 24132, 7103, 29892, 530, 858, 403, 29892, 8278, 891, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29955, 891, 395, 29896, 29929, 29929, 29889, 29929, 29955, 891, 7089, 277, 9160, 891, 2]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29989, 512, 14917, 396, 891, 21886, 4408, 891, 8170, 4712, 891, 8170, 25085, 891, 6682, 292, 16428, 891, 24323, 891, 14990, 891, 14617, 358, 21864, 891, 13, 29989, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 11474, 891, 13, 29989, 29871, 29900, 29900, 29896, 891, 2259, 7075, 891, 5468, 29871, 29906, 299, 29892, 29871, 29906, 29900, 29906, 29941, 891, 323, 29899, 845, 2728, 313, 18700, 29892, 3436, 1974, 511, 2581, 550, 313, 21319, 29892, 29871, 29941, 29946, 511, 317, 686, 605, 267, 313, 29934, 388, 29899, 29933, 273, 29897, 891, 29871, 29896, 29906, 29941, 4241, 7103, 29892, 3139, 27734, 29892, 8278, 891, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29945, 891, 395, 29896, 29955, 29900, 29889, 29929, 29955, 891, 24596, 277, 9160, 891, 13, 29989, 29871, 29900, 29900, 29906, 891, 10447, 1938, 29872, 891, 5468, 29871, 29945, 386, 29892, 29871, 29906, 29900, 29906, 29941, 891, 360, 1253, 313, 9039, 29892, 18285, 511, 382, 2749, 886, 313, 29968, 403, 1706, 1943, 29897, 891, 29871, 29946, 29945, 29953, 7315, 280, 18874, 29892, 3139, 12690, 29892, 8278, 891, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29953, 891, 395, 29929, 29945, 29889, 29900, 29900, 891, 14617, 18210, 891, 13, 29989, 29871, 29900, 29900, 29941, 891, 5765, 11717, 891, 5468, 29871, 29929, 386, 29892, 29871, 29906, 29900, 29906, 29941, 891, 19509, 17550, 267, 313, 29940, 9345, 29892, 29871, 29896, 29900, 511, 323, 29899, 845, 2728, 313, 29954, 8903, 29892, 8218, 479, 511, 435, 468, 3460, 349, 1934, 313, 3253, 8817, 29892, 3436, 1974, 29897, 891, 29871, 29955, 29947, 29929, 24132, 7103, 29892, 530, 858, 403, 29892, 8278, 891, 29871, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29899, 29945, 29945, 29945, 29955, 891, 395, 29896, 29929, 29929, 29889, 29929, 29955, 891, 7089, 277, 9160, 891, 2]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Retraining Model : \nmodel,trainer = My_ODIE.Training_Model(train_data,val_data,batch_size = 4, num_epochs = 10, learning_rate = 1e-4,\n                              gradient_accumulation_steps = 16, output_dir = 'Model_Output_Train_V2'\n                              )","metadata":{"execution":{"iopub.status.busy":"2024-07-01T04:23:44.703351Z","iopub.execute_input":"2024-07-01T04:23:44.703665Z","iopub.status.idle":"2024-07-01T15:33:38.622623Z","shell.execute_reply.started":"2024-07-01T04:23:44.703639Z","shell.execute_reply":"2024-07-01T15:33:38.621286Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"========================Please Waiting==============================\n====================================================================\n========================Loading Model===============================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"526b8cf0c00f4864a158856828915c99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a82e80f7e764c4faec6e63fbb4ef853"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370702b2b7b4437ab2e3e08c563ccdd7"}},"metadata":{}},{"name":"stdout","text":"Training From First!\n================Information Model=====================\n======================================================\ntrainable params: 13,705,216 || all params: 1,113,753,600 || trainable%: 1.2305\n","output_type":"stream"},{"name":"stderr","text":"2024-07-01 04:24:05.701918: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-01 04:24:05.702046: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-01 04:24:05.835321: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"======================================================\nCompile Model with Torch :\n======================================================\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"======================Training============================\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240701_042428-noitzt91</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/huflit/huggingface/runs/noitzt91' target=\"_blank\">Model_Output_Train_V2</a></strong> to <a href='https://wandb.ai/huflit/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/huflit/huggingface' target=\"_blank\">https://wandb.ai/huflit/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/huflit/huggingface/runs/noitzt91' target=\"_blank\">https://wandb.ai/huflit/huggingface/runs/noitzt91</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [560/560 11:07:39, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.181200</td>\n      <td>0.188827</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.150300</td>\n      <td>0.171136</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.120000</td>\n      <td>0.166941</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.083400</td>\n      <td>0.179074</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.066400</td>\n      <td>0.192207</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.048100</td>\n      <td>0.213158</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.029000</td>\n      <td>0.262217</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.024000</td>\n      <td>0.272119</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"name":"stdout","text":"========================Saving==============================\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"name":"stdout","text":"Training Model Completion!\n","output_type":"stream"}]},{"cell_type":"code","source":"#Testing : \ntokenizer = My_ODIE.tokenizer\ninputs = []\nfor i in test_data['input']:\n    tokenize = tokenizer(i, \n                       padding = True,\n                       truncation = True,\n                       return_tensors=\"pt\",\n                       max_length=2048\n                      ).input_ids.to('cuda')\n    inputs.append(tokenize)\n    \ninputs","metadata":{"execution":{"iopub.status.busy":"2024-07-01T15:45:46.880509Z","iopub.execute_input":"2024-07-01T15:45:46.880895Z","iopub.status.idle":"2024-07-01T15:45:46.946844Z","shell.execute_reply.started":"2024-07-01T15:45:46.880867Z","shell.execute_reply":"2024-07-01T15:45:46.945976Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n           3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n          14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n          29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  2744,\n          14997,   911,  1438, 16200,  8324,   304, 24809, 16200, 12554,  3335,\n          29889,  7338,  1461,  8018,   848, 29892,  3704,   278,  2635,   310,\n           2280, 29892, 16200,  8158, 29892, 16200,   297,  6578,  2722, 29892,\n          24806,  5253, 29892,   322, 17346, 29889,    13,    13,  2277, 29937,\n          10567, 29901,    13, 29896, 29889, 11639,  7075, 29892,  6345,   373,\n           5490, 29871, 29896, 29892, 29871, 29896, 29929, 29947, 29945, 29892,\n            756,   263, 16200,  8158,   310, 29871, 29955, 29906, 29900, 29892,\n          23941,  1781, 16200, 29889,  3600, 16200,  4955,  7805, 24596,   277,\n           9160,   319, 29892,  6496,   373,  5490, 29871, 29896, 29892, 29871,\n          29906, 29900, 29900, 29945, 29892,   411,   263, 16200,  4046,   310,\n            395, 29896, 29900, 29892, 29900, 29900, 29900,   322,   263,  1857,\n          17346,   310,   395, 29941, 29892, 29900, 29900, 29900, 29892,   411,\n          13747,   373, 29899,  2230,  5146,  1860, 29889,   940,   884,   756,\n          24596,   277,  9160,   350, 29892,  6496,   373,  4779, 29871, 29896,\n          29892, 29871, 29906, 29900, 29896, 29900, 29892,   411,   263, 16200,\n           4046,   310,   395, 29945, 29892, 29900, 29900, 29900,   322,   263,\n           1857, 17346,   310,   395, 29900, 29892,   411,  4943,   373, 29899,\n           2230,  5146,  1860, 29889,  2259,   756,   385,  4469, 24806,  7625,\n            373,  5306, 29871, 29896, 29892, 29871, 29906, 29900, 29896, 29906,\n          29892,   363,   263,  3001,  5253,   310,   395, 29906, 29900, 29892,\n          29900, 29900, 29900, 29892,   411,   263,  9886, 17346,   310,   395,\n          29945, 29892, 29900, 29900, 29900,   322,  4943,  4098,   368,  5146,\n           1860, 29889,   940, 16692,   263,  5758, 29887,   482,   373,  3111,\n          29871, 29896, 29892, 29871, 29906, 29900, 29896, 29945, 29892,   363,\n            263,  2875,  7088,   395, 29941, 29900, 29900, 29892, 29900, 29900,\n          29900, 29892,   411,   263,  1857,   714, 11235, 17346,   310,   395,\n          29906, 29945, 29900, 29892, 29900, 29900, 29900,   322,   373, 29899,\n           2230,  4098,   368,  5146,  1860, 29889, 19814, 29892,   540,   756,\n            263,  8368, 24806,  4586,   714,   373,  3839, 29871, 29896, 29892,\n          29871, 29906, 29900, 29900, 29953, 29892,   411,   263,  9886, 17346,\n            310,   395, 29896, 29900, 29892, 29900, 29900, 29900,   322,  4943,\n           4098,   368,  5146,  1860, 29889,  1670,   505,  1063, 16200,   297,\n           6578,  2722,   363,   385,  4469, 24806,   373,  3979, 29871, 29896,\n          29892, 29871, 29906, 29900, 29906, 29900, 29892,   263, 16200,  5881,\n           2280,   373,  6339, 29871, 29896, 29945, 29892, 29871, 29906, 29900,\n          29906, 29906, 29892,   322,   263,  5758, 29887,   482,  2280,   373,\n           5468, 29871, 29896, 29900, 29892, 29871, 29906, 29900, 29896, 29945,\n          29889,  1939, 16250, 29892,  9124,  6685,  2478, 29892,   619,   575,\n          29892,  6577, 29887,  1860, 29892,   470,   628,   262, 16011, 15303,\n            526,  8967,   297,   670,   970,  6475, 29889,    13, 29906, 29889,\n           6026,  2354, 11717, 29892,  6345,   373,  2610, 29871, 29896, 29900,\n          29892, 29871, 29896, 29929, 29929, 29900, 29892,   756,   263, 16200,\n           8158,   310, 29871, 29953, 29947, 29900, 29892, 23941,   263,  6534,\n          16200, 21700, 29889,  2439, 16200,  4955,  3697,  1023,  6136, 16200,\n          15889,   411,  6411,  2925,   322, 19179,  6475, 29889, 24596,   277,\n           9160,   319,   471,  6496,   373,  5468, 29871, 29896, 29892, 29871,\n          29906, 29900, 29896, 29906, 29892,   411,   263, 16200,  4046,   310,\n            395, 29945, 29892, 29900, 29900, 29900,   322,   263,  1857, 17346,\n            310,   395, 29906, 29892, 29945, 29900, 29900, 29889, 23089,  5146,\n           1860,   505, 10761, 23025,   297,   278,  4940, 29889, 24596,   277,\n           9160,   350, 29892,  6496,   373,  3839, 29871, 29896, 29945, 29892,\n          29871, 29906, 29900, 29896, 29945, 29892,   756,   263, 16200,  4046,\n            310,   395, 29941, 29892, 29900, 29900, 29900,   322,   263,  1857,\n          17346,   310,   395, 29896, 29892, 29900, 29900, 29900, 29892,   411,\n          13747,   373, 29899,  2230,  5146,  1860, 29889,  2812,  2354,   884,\n            756,   385,  6136,  7333, 24806,   322,   385,  4469, 24806, 29892,\n            411,  4943,  4098,   368,  5146,  1860,   322,  9886,  6411,  2925,\n          29889,  2398, 29892,  1183,   756,   263,  4333,  3633,   363,   385,\n            443,  3274,   333, 16083, 11118,   310,   395, 29945, 29900, 29900,\n          29892,  8967,   373,  5490, 29871, 29896, 29900, 29892, 29871, 29906,\n          29900, 29906, 29906, 29889,  1939,  9124,  6685,  2478, 29892,   619,\n            575, 29892,   470,  6577, 29887,  1860,   526,  8967,   297,   902,\n            970,  6475, 29889,  2812,  2354,   756,   750,  7786, 16200,   297,\n           6578,  2722,   363,   385,  4469, 24806, 29892,   263, 16200,  5881,\n          29892,   322,   263,  7333, 24806, 29889,    13, 29941, 29889, 24083,\n          22348,   756,   263, 16200,  8158,   310, 29871, 29955, 29947, 29900,\n          29892,   607,   338,  5545, 15129, 29889,  3600, 16200,  4955,  3697,\n          14040, 10643,   310,   670, 16200, 15303, 29889,   940,   756, 24596,\n            277,  9160,   319,   411,   263, 16200,  4046,   310,   395, 29896,\n          29945, 29892, 29900, 29900, 29900,   322,   263,  1857, 17346,   310,\n            395, 29906, 29892, 29900, 29900, 29900, 29892,   607,   540,  5718,\n           2705,  9744,   373,   931, 29889, 24596,   277,  9160,   350,   756,\n            263, 16200,  4046,   310,   395, 29947, 29892, 29900, 29900, 29900,\n            322,   263,  5225, 17346, 29892, 23941,  4943,   373, 29899,  2230,\n           5146,  1860, 29889,  5765,   884,   756,   385,  4469, 24806,   411,\n            263,  9886, 17346,   310,   395, 29896, 29900, 29892, 29900, 29900,\n          29900,   322,   263,  5758, 29887,   482,   411,   385,   714, 11235,\n          17346,   310,   395, 29941, 29900, 29900, 29892, 29900, 29900, 29900,\n          29892,  1716,   310,   607,   540,  9744,  4098,   368,  1728,   738,\n           5626, 29889, 19814, 29892,   540,   756,   263,  8368, 24806,   411,\n            263,  9886, 17346,   310,   395, 29945, 29892, 29900, 29900, 29900,\n          29889,  5765,   756,  1754, 16200,   297,  6578,  2722,   363,  2143,\n            262, 19985,   670,  4469, 24806, 29892, 15399,   363,   263,   716,\n          16200,  5881, 29892,   322,  3902,  8253,  7333, 24806,  3987,   363,\n           2553, 29873,  1136, 17211,   362, 29889,  1670,   526,   694, 16250,\n          29892,  9124,  6685,  2478, 29892,   619,   575, 29892,   470,  6577,\n          29887,  1860,  8967,   297,   670,   970,  6475, 29889,  6811,   497,\n          29892,  5765,   756,   263,  4549, 16200,  4955,   322,  9004,  1078,\n          14040, 16200, 10643, 29889,    13,    13,  2277, 29937, 13291, 29901,\n             13]], device='cuda:0'),\n tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n           3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n          14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n          29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 23323,\n            366,  3113, 27599,   278,  8608,   362, 10748,   322,  6597,  2472,\n          11211, 28289,  6155, 29892, 12049, 29892,   322, 19716,  5855, 29973,\n            910,   848,   338, 12187,   363,  5994,  5281,  1749,  1480,  6765,\n           6931,   322,  4857,  1747, 28289, 28598,  2540, 29889,    13,    13,\n           2277, 29937, 10567, 29901,    13, 29896, 29889, 19658,  4522,  6765,\n           8608,   287,   263,  7751,   358,   373,  5306, 29871, 29896, 29892,\n          29871, 29906, 29900, 29906, 29941, 29892,   773,   323, 10857, 29871,\n          29896, 29906, 29941, 18225,   491,  2259, 11717, 29889,   450, 17487,\n           4687,   472,   399,   598,  8697,   319,   322,  9698,   472, 21886,\n           1060, 29892, 21653,   263,  5418,   310, 29871, 29896, 29900, 29900,\n           7800, 29889, 23242,   545,   931,   471,   472, 29871, 29929, 29901,\n          29900, 29900, 13862, 29892,   322, 18517, 10761,   472, 29871, 29896,\n          29896, 29901, 29941, 29900, 13862, 29889,   450, 17040, 29892, 19849,\n            310, 29871, 29906, 29900,  5112, 10376,   310, 11966,  1199, 29892,\n           9488,   938,   627, 10106,   278, 16342, 29889,   450,  8608,   362,\n           3614,  2058,  1090,  6575,  1460, 14826,  5855,   411,   694,  8967,\n           6520,   447, 29920,  3163, 29889,   450, 25373,  5782,   471, 22080,\n          29871, 29896, 29892,  5622, 25954, 29871, 29945,   304, 25954, 29871,\n          29896, 29900, 29889,   450,  7156,  3614,   263, 29871, 29941, 29900,\n          29899,  1195,  1082,  1791,  2867,   472, 11654, 18320,   350,  2820,\n          29871, 29896, 29900, 29901, 29941, 29900, 13862, 29889,   450, 19716,\n           1090, 29893,   296, 26529, 25413,   373,  2610, 29871, 29906, 29945,\n          29892, 29871, 29906, 29900, 29906, 29941, 29892,   322,   694,  5528,\n          16719,   470,  1035, 16719,   892,  8967,  2645,   278, 17487, 29889,\n             13, 29906, 29889,  2951,  5306, 29871, 29896, 29900, 29892, 29871,\n          29906, 29900, 29906, 29941, 29892,  1060, 29979, 29999,  1383, 17347,\n          29915, 29879,   323, 10857, 29871, 29955, 29947, 29929, 29892, 18225,\n            491, 19235,  7075, 29892,  8676,   263,  8608,   362, 17487, 29889,\n            450, 16342,  4687,   472,   399,   598,  8697,   350,   322,  9698,\n            472, 21886,   612, 29889, 23242,   292,   472, 29871, 29947, 29901,\n          29941, 29900, 13862,   322,  6974,   292,   472, 29871, 29906, 29901,\n          29900, 29900, 11278, 29892,   278,   534,  2707,  1020,   345,   839,\n            263,  5418,   310, 29871, 29906, 29945, 29900,  7800, 29892,  1136,\n           9929, 29871, 29906, 29900, 11798,   787,   310, 26413, 29889,   450,\n          17040, 24775,   310, 29871, 29941, 29900,  2181,  1078,   310,  1374,\n          22824,   346,   329,   936, 28075, 29892,   607,   892,  8967,   304,\n            367,   297,   938,   627,  4195, 29889,   450, 17487,  3614,  2058,\n           1090, 22669,  9570, 29891,  2071,   583,   411,  3578, 17251,  1510,\n            414, 29889,   450, 25373,  5782,   471,  4124,  3859, 29871, 29929,\n          29945, 29892, 10816,   515, 25954, 29871, 29896, 29945,   304, 25954,\n          29871, 29906, 29900, 29889,  7133,   278, 17487, 29892,   278,  7156,\n           3614,   263, 29871, 29946, 29945, 29899,  1195,  1082,  2867,   472,\n          11654, 18320,   315,  2820, 29871, 29896, 29896, 29901, 29900, 29900,\n          13862, 29889,   450, 19716,   750,  1090, 29887,   650, 26529, 25413,\n            373,  5306, 29871, 29945, 29892, 29871, 29906, 29900, 29906, 29941,\n          29892,   322,   694,  5528, 16719,   470,  1035, 16719,   892,  8967,\n           2645,   278, 16342, 29889,    13, 29941, 29889,  2951,  5306, 29871,\n          29896, 29945, 29892, 29871, 29906, 29900, 29906, 29941, 29892,  1060,\n          29979, 29999,  4522,  6765,  8608,   287, 29871, 29896, 29945,  2181,\n           1078,   310,   639,   728,   519, 22535,   515,   399,   598,  8697,\n            315,   304, 21886,   796,   773,   323, 10857, 29871, 29946, 29945,\n          29953, 29889,   450, 17487,  4689,   472, 29871, 29896, 29900, 29901,\n          29900, 29900, 13862,   322,  9698,   472, 29871, 29906, 29901, 29941,\n          29900, 11278, 29892, 21653,   263,  5418,   310, 29871, 29896, 29945,\n          29900,  7800, 29889,   450, 26413, 27430,   471, 29871, 29896, 29906,\n          11798,   787, 29892,   322,   278, 17040,  9488,   938,   627, 10106,\n            278, 16342, 29889,   450, 14826,  5855,   892,   975,  4384,   411,\n          29574,  1510,   414, 29892,   541,   694,  8967,  6520,   447, 29920,\n           3163,   892, 18169, 29889,   450, 25373,  5782,   471, 22080, 29871,\n          29941, 29892,  5622, 25954, 29871, 29947,   322,  6876,   292,   472,\n          25954, 29871, 29896, 29906, 29889,   450,  7156, 29892,  2812,  2354,\n          15225, 29892,  3614,   263, 29871, 29941, 29900, 29899,  1195,  1082,\n           2867,   472, 11654, 18320,   360,  2820, 29871, 29896, 29906, 29901,\n          29900, 29900, 11278, 29889,   450, 19716,   750,  1090, 29887,   650,\n          26529, 25413,   373,  5306, 29871, 29896, 29900, 29892, 29871, 29906,\n          29900, 29906, 29941, 29892,   322,   694,  5528, 16719,   470,  1035,\n          16719, 10761,  2645,   278, 17487, 29889,    13,    13,  2277, 29937,\n          13291, 29901,    13]], device='cuda:0'),\n tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n           3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n          14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n          29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 29902,\n          30010, 29885,  3063,   304,   679, 14432,   310,   278, 13460,   534,\n           1975, 29889,  6527,   366,  1371,   592, 11705,   848,   373,  1067,\n           6046, 11949, 29892, 17279, 29892,   322, 15038,   515,  1438, 13460,\n          12618, 29879, 29973,   306,   864,   304,  1073,   825, 29915, 29879,\n           5517,   304,   367,   297,   325,   468,   434,  4720, 29889,    13,\n             13,  2277, 29937, 10567, 29901,    13,  1576,  6480,   310,   278,\n          20084,   515, 13851,   304,  6709,   338,   263,  1556,  5566, 11407,\n            931,  7148,   269,   442,  9020,   368, 29889,   512,   411,   301,\n          14643, 15359, 29892, 10714,   267, 29892,   322,   263,  4060,   310,\n          16082, 29936,   714,   411,  9416, 11420,   705,   279, 29892,   889,\n           1169, 29892,   322,  1045,  1862, 29889,  1932,   372,  5304,   304,\n            278,  6709, 29871, 29906, 29900, 29906, 29941,   534,  1975, 29892,\n            727, 30010, 29879,   263,  3287,   304,  1106,  6375,   304, 29889,\n           1094, 11682,   491, 20797,   434,  7525,  1582, 30010, 29879,   534,\n            355,  3461, 29892,   278, 16250, 25312,  1784,   286,  2092, 29879,\n          29901,   515,  4964,   322,   563,   650,   313,   949,   270,  2390,\n            287,   432,   261,  7759, 29892,  1320,  1253,   287, 10135, 10817,\n          29892,   322,   916,  6023,   519,  1426,  1973, 29897,   304,  3667,\n           3673,   713, 13240,  2264,   313,   386,   682,  1532, 29899,  1129,\n           3522,   287,  7171,  1860, 29897,   322,   263,  4840, 21354, 13114,\n           1608,   313,  4149, 15040, 12464,  8253,   322,  1584,  1560,  4254,\n           2826, 29899,   786,   528,   381,  1259,   467,  8680,  3863,   943,\n            505,  2307, 23580,   304,  8310, 10388,   304,   263,  1361,  1319,\n            310,  1438, 29892, 19888,   304,  9558,   304,   278,  6709,   281,\n            538,   307,   915, 29889,    13,    13,  6833,   549,  1438,   534,\n           1975,   526,   263,  4060,   310, 15031,   537,   322,   385, 19310,\n            275,   373,   263, 14218,  9090, 29889,  7803,  3863,   943,   526,\n          20794,  1009,  4236, 29875,  2071,   381,  1372,   322, 12464,  4395,\n            325,  9197,   964,  6709,   411,   263, 10849, 18520, 20603,   491,\n            278,  3697, 29892,  1550,  4045,   526,  3063,   363,   716,  5837,\n            304, 19531,   470,  1634,   332,  4220,  1906,  1556, 14954,  3276,\n            770,  1199, 29901,   972,   326,   470,   278,  4796, 10714,   528,\n           2728, 29892,   363,  1342, 29889,  7419,   304,   445,   528,  9354,\n            519,  6709, 29871, 29906, 29900, 29906, 29941,   534,   355, 10754,\n            363,   385,  2768,  1106,   472,   825,  1749,  3863,   943,   526,\n          17394,  3262,  1286,   322,  1556, 24173,  1048, 29889,    13,    13,\n           2277, 29937, 13291, 29901,    13]], device='cuda:0'),\n tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n           3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n          14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n          29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 29902,\n           6091,   366, 29915,   276,  2107,   472,  4697,  3460,  1549,   848,\n          29889,   306, 29915, 29885,  1985,   373,   445, 24201,  2060,   322,\n           1033,   671,   777,  1371, 29889,  1815,   366,   679,   592,   777,\n           5235,   373,  6606,  4978,   322,  4665,  3694,   515, 16021,  7456,\n            322,  1746, 13676, 29973,    13,    13,  2277, 29937, 10567, 29901,\n             13, 29911,  7358,   526, 22611,   278,  4100,   321,   312,   459,\n            279,   294,  3246,   988,  7294,   342,  1698,   526, 15041, 29892,\n            408,   896, 19901,   873,  6602,   278, 15006,  1549, 10416,  2146,\n            384,   292, 29889,   512, 21881,   322, 12059,  1336,   936, 10916,\n          29892,   896, 22649,  2224,   468,   575,  1316,   408,   289,   370,\n            267,  2363,   275, 29892,   278,  3955,  2363,   275, 29892,   321,\n           1092,  1470,  2363,   275, 29892,   322,   385,   481,  3333,  7681,\n            275,   297, 27498, 29892, 10805,   263, 20376,   297,  5802,  6554,\n            322,  7282,   378,   510,   277,   424, 17407, 28495, 29889,   323,\n           7358,  6602, 29871, 29947, 29900, 29995,   310,   278, 27498,  4665,\n           4822,   278,  3186, 29892,   411,   385, 15899, 17407,  6410,   310,\n           3148, 29928, 29871, 29906, 29900, 29994, 29941, 29900, 24464,   639,\n           1629, 29889,   512,  4275, 10557, 29892, 17407, 28495,   297,   278,\n           7294,   342,  1698, 13661,  8581,   491,   260,  7358,   322, 16892,\n          29899,  4089,   484, 10267,  2129,   526, 15899,   304, 13461,  3148,\n          29928, 29871, 29941, 29941,  7284,   639,  1629,   313, 29999,  1718,\n          29871, 29945, 29900, 29900,  7284,   467,  1670,   526,  9881,  4655,\n           1176, 29874,   310,   474, 29916,   397,   333,   260,  7358,   297,\n          14234, 10557,   313, 29875, 29889, 29872,  1696, 15145,   368,   290,\n            655, 29892,   360,   837, 16648,   272, 29892,  5952,   331, 12451,\n            952,  5711, 29892,   379,  4605,   290,   655, 29892,   306, 29916,\n           2631,   322,   390,  4034,   625, 17206,   375,   467,   450,  5177,\n            297,   607,   263, 16892, 12080,   338,  1754,   701,   310,   599,\n            278,  5164,  4768,  5996,   322,   633, 29875, 13574, 13879,   393,\n            526,  2845,  5181,   470, 19039,   363,   967,  2834, 29889,   450,\n          10161,   988,  5164,   260,  7358,   505,  1063,  1476,   505,  1063,\n          23531,   297,  1784, 25964, 29889,  5293,  1438,   848, 29892, 11053,\n            310,  1950,  6606, 30010,  4760,  1446,   508,   367,  1754, 29889,\n          20315,  6475,   373, 16892,  4978,  1122,   367, 10240,  2861,   304,\n          29769, 28947,   470,   263,  1735,   297,   278, 16892, 30010, 29879,\n           1024, 29889,  2178,   278,  8974,  1304,   304,  5706,   278, 11053,\n            363,   445,  9076,   892,   443,  5467,  3726,   322,  2996,   515,\n            263,  9377,  3464,   310,  8974, 29889,  1763, 12439, 16892,  6606,\n            322,   278, 10259,  1384,   292,  9200, 29890,   616,   321,  3944,\n            973, 29879, 29892,  5925,   414,   526, 10231,   368,  9332,   292,\n          16892, 29769,  3519,  3704, 29871, 29896, 29953, 29903,   322, 29871,\n          29896, 29947, 29903,   364, 29928,  3521, 18530,  8617, 16750, 29889,\n          14598, 29892,  2217,   338,  2998,  1048,   278,  2531,  7492, 10551,\n            800,   393,  2367,  4100,  1020,  1169, 29892,  3704,   278,  4450,\n            488,   428,   363, 16892, 18982, 29892, 22713, 29892,   322,  1274,\n            279,   293,   680, 17711, 29889,   438,  3016,   348,  1907,   363,\n           3902,  8253,  1438,  3620,   297, 16892, 23093,   322,  1014,  7323,\n           8250,   526,  4944,   491,  3061,  4564,  4110,   297,  2703,  1199,\n           5722, 11763, 29889,   450, 12845,   373,   278, 12875,   310,   474,\n          29916,   397,   333,   260,  7358, 29892,  1009,  1513,   322, 26377,\n           9545, 29892,   322,  2761,  3519,   297,  4275, 10557,   338, 13126,\n            297,   445,  9076, 29889,    13,    13,  2277, 29937, 13291, 29901,\n             13]], device='cuda:0'),\n tensor([[    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29892,\n           3300,  2859,   411,   385,  1881,   393,  8128,  4340,  3030, 29889,\n          14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n          29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 29909,\n          26127,   403,  1855, 29899,  2230,   848,   515,  5264,  5745, 29892,\n           9763,   714, 10376, 29892,   322, 14826,  5786,  2645,   385,  8437,\n            339,  1296, 29889,   512,  2325,  2472,   373, 18497, 29892,  2635,\n          29892,  4423, 29892,   322, 10879,   304, 17583, 14821,  2933, 14231,\n          29889,    13,    13,  2277, 29937, 10567, 29901,    13,  2951, 26319,\n          29889, 29871, 29953, 29892,   263, 18497, 29871, 29955, 29889, 29947,\n           8437,   339,  1296, 10761,   297, 14841, 26459,  2978,   278, 14622,\n           5139,   310,  8713,  2849, 29889,   910,   439,  1296,   471,  5643,\n          14235, 14183,  6199,  2678,   491,   263, 18497, 29871, 29955, 29889,\n          29945,  8437,   339,  1296,  5982,  2820, 29871, 29945, 29929,  7800,\n            313, 29929, 29945, 20052,  2699, 29897,   304,   278,  7062,  5933,\n          29889,  1576,   937,  8437,   339,  1296,   471,   278,  1556,  2906,\n            579,  1218,   304,  7124,  8437,   339,  1296, 29899,   558,   650,\n          26459,   297,   901,  1135, 29871, 29906, 29900,  2440,   322,   471,\n            408,  4549,   408,   697,   297, 29871, 29896, 29929, 29941, 29929,\n          29892,   278,  1556, 13988, 10478,   727, 29889,   739,   471, 24764,\n           2978, 15853, 12361, 29886,   297,  7062, 29899, 25171, 26459, 29892,\n           3271,   304, 17202,   310,  8713,  6392, 25447,   267,   322,   278,\n           1784,  5199,  3673,   713, 16226, 25700,   884,  2729,   727, 29889,\n           1576, 24682,  5874,   338,  8236,   278,  2933,   727,  1549, 29311,\n           3381,   491, 23844,  3035,   322,   411,   278, 24682,  4367,   315,\n            690,  1760, 29889,  4306, 21142,  8052,   263,  3233, 29899, 29946,\n          11176, 14703,  8236,   304,   263,  1246,   363,  6121, 18872, 29889,\n          24682,  7178,   830, 13300,   323,   388, 29891,   666,  1425, 26169,\n            273,  8052,   263,  2211, 29899, 10874,  2106,   310, 11176, 14703,\n            297, 29871, 29896, 29900,   310,   278,  4234, 30010, 29879, 28058,\n          29889, 29954,  6170,  1860,  2820,   278,  3186,   892,  4996,   304,\n          10049,   304,  7274,   363,  6121, 18872, 29892,  7246,   292, 26429,\n          10907,   322, 27032, 16226, 29889,   450,  4234,   310, 26459,   338,\n          14831,   297,  4223,   408, 25424,  1984,  4099,   491,   278,  3303,\n          18269,   313,  3904,   467,    13,    13,  2277, 29937, 13291, 29901,\n             13]], device='cuda:0')]"},"metadata":{}}]},{"cell_type":"code","source":"generation_config = GenerationConfig(\n            do_sample=True,\n            temperature=0.1,\n            top_p=0.75,\n            top_k=40,\n            num_beams=4,\n        )\n\nmodel.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\nmodel.config.bos_token_id = 1\nmodel.config.eos_token_id = 2","metadata":{"execution":{"iopub.status.busy":"2024-07-01T15:33:38.713177Z","iopub.execute_input":"2024-07-01T15:33:38.713457Z","iopub.status.idle":"2024-07-01T15:33:38.721641Z","shell.execute_reply.started":"2024-07-01T15:33:38.713434Z","shell.execute_reply":"2024-07-01T15:33:38.719252Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model.eval()\nmodel.to('cuda')\noutputs = []\n# model.to('cuda')\nfor inp in inputs :\n    with torch.no_grad():\n        generation_output = model.generate(\n                                input_ids=inp,\n                                generation_config=generation_config,\n                                return_dict_in_generate=True,\n                                output_scores=True,\n                                max_new_tokens=1025,\n                            )\n    outputs.append(generation_output.sequences)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T15:53:13.866753Z","iopub.execute_input":"2024-07-01T15:53:13.867602Z","iopub.status.idle":"2024-07-01T15:57:36.782670Z","shell.execute_reply.started":"2024-07-01T15:53:13.867569Z","shell.execute_reply":"2024-07-01T15:57:36.781502Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"outputs","metadata":{"execution":{"iopub.status.busy":"2024-07-01T15:57:36.784557Z","iopub.execute_input":"2024-07-01T15:57:36.784853Z","iopub.status.idle":"2024-07-01T15:57:36.795740Z","shell.execute_reply.started":"2024-07-01T15:57:36.784810Z","shell.execute_reply":"2024-07-01T15:57:36.794784Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"[tensor([[    1, 13866,   338,  ..., 29945, 29900, 29900]], device='cuda:0'),\n tensor([[    1, 13866,   338,  ..., 29958,    13,  1678]], device='cuda:0'),\n tensor([[    1, 13866,   338,  ..., 29900, 29889, 29900]], device='cuda:0'),\n tensor([[    1, 13866,   338,  ...,    13,  1678,   529]], device='cuda:0'),\n tensor([[    1, 13866,   338,  ..., 29900, 29906, 29896]], device='cuda:0')]"},"metadata":{}}]},{"cell_type":"code","source":"#Decode Generate :    \ndecode = []\nfor i in outputs:\n    decode.append(tokenizer.decode(i[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T16:03:09.662198Z","iopub.execute_input":"2024-07-01T16:03:09.662582Z","iopub.status.idle":"2024-07-01T16:03:09.679774Z","shell.execute_reply.started":"2024-07-01T16:03:09.662551Z","shell.execute_reply":"2024-07-01T16:03:09.678892Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Set output with template : \ndecode_pr = []\nfor d in decode:\n    decode_pr.append(My_ODIE.Get_Response(d))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T16:03:18.992007Z","iopub.execute_input":"2024-07-01T16:03:18.992740Z","iopub.status.idle":"2024-07-01T16:03:19.027722Z","shell.execute_reply.started":"2024-07-01T16:03:18.992709Z","shell.execute_reply":"2024-07-01T16:03:19.026741Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"for i in decode_pr :\n    print('Output : ')\n    print(i)\n    print('='*80)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T16:06:01.074025Z","iopub.execute_input":"2024-07-01T16:06:01.074376Z","iopub.status.idle":"2024-07-01T16:06:01.083464Z","shell.execute_reply.started":"2024-07-01T16:06:01.074348Z","shell.execute_reply":"2024-07-01T16:06:01.082460Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Output : \n| Credit Score | Date of Application | Credit Inquiries | Loan Amount | Balance |\n| --- | --- | --- | --- | --- |\n| 720 | January 1, 2005 | 1 | $10,000 | $3,000 |\n| 720 | March 1, 2010 | 1 | $5,000 | $0 |\n| 720 | June 1, 2012 | 1 | $20,000 | $5,000 |\n| 720 | August 1, 2015 | 1 | $250,000 | $250,000 |\n| 720 | September 1, 2006 | 1 | $10,000 | $10,000 |\n| 720 | November 1, 2020 | 1 | $10,000 | $10,000 |\n| 720 | July 10, 2015 | 1 | $300,000 | $250,000 |\n| 680 | May 10, 1990 | 1 | $5,000 | $2,500 |\n| 680 | July 1, 2012 | 1 | $5,000 | $2,500 |\n| 680 | September 15, 2015 | 1 | $3,000 | $1,000 |\n| 680 | January 1, 2005 | 1 | $15,000 | $2,000 |\n| 680 | March 1, 2010 | 1 | $5,000 | $0 |\n| 680 | June 1, 2012 | 1 | $20,000 | $5,000 |\n| 680 | August 1, 2015 | 1 | $250,000 | $250,000 |\n| 680 | September 1, 2006 | 1 | $10,000 | $10,000 |\n| 680 | November 1, 2020 | 1 | $10,000 | $10,000 |\n| 680 | July 10, 2015 | 1 | $300,000 | $250,000 |\n| 620 | May 10, 1990 | 1 | $5,000 | $2,500 |\n| 620 | July 1, 2012 | 1 | $5,000 | $2,500 |\n| 620 | September 15, 2015 | 1 | $3,000 | $1,000 |\n| 620 | January 1, 2005 | 1 | $15,000 | $2,000 |\n| 620 | March 1, 2010 | 1 | $5,000 | $0 |\n| 620 | June 1, 2012 | 1 | $20,000 | $5,000 |\n| 620 | August 1, 2015 | 1 | $250,000 | $250,000 |\n| 620 | September 1, 2006 | 1 | $10,000 | $10,000 |\n| 620 | November 1, 2020 | 1 | $10,000 | $10,000 |\n| 620 | July 10, 2015 | 1 | $300,000 | $250,000 |\n| 560 | May 10, 1990 | 1 | $5,000 | $2,500\n================================================================================\nOutput : \n| Delivery Datas | Routes | Vehicle Conditions |\n| --- | --- | --- |\n| June 1, 2023 | Warehouse A to Customer X | Truck 123 driven by John Johnson |\n| June 10, 2023 | Warehouse B to Customer Y | Truck 789 driven by Sarah Smith |\n| June 15, 2023 | Warehouse C to Customer Z | Truck 456 driven by Emily Davis | <repositories>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>https://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>https://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>https://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>https://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>https://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>https://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>https://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>https://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>https://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>https://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>https://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>https://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>https://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>https://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>https://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>https://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>https://repo.spring.io/snapshot</url>\n    </repository>\n================================================================================\nOutput : \n| Fashion Blog | Clothing Styles | Materials | Patterns |\n| --- | --- | --- | --- |\n| Vogue Runway | Draped jersey, distressed fabrics, other touchable textures | Soft and undone | Utilitarian preparedness, well-pocketed garments, smart tailoring, even smarter button-up shirting |\n| The Fashion Edit | Maxi skirts, tailored vests, denim, white dress shirt | Maxi skirts, tailored vests, denim, white dress shirt | Maxi skirts, tailored vests, denim, white dress shirt | <filename>README.md<gh_stars>1-10\n# 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0\n================================================================================\nOutput : \n| Species Distribution and Population Numbers | Source |\n| --- | --- |\n| Ticks | Scientific articles and field reports |\n| Ixodid Ticks | Seven major genera of ixodid ticks in Southern Africa (i.e., Amblyomma, Dermacentor, Haemaphysalis, Hyalomma, Ixodes and Rhipicephalus) |\n| Tick Habitat | Historical records on tick distribution may be incorrect due to identification mistakes or a change in the tick's name |\n| Microbial Ecosystems | Researchers are increasingly adopting tick identification methods including 16S and 18S rDNA gene sequencing |\n| Genetic Alterations | Little is known about the genetic alterations that give important traits, including the predilection for tick hosts, transmission, and acaricide resistance |\n| Opportunities for Exploring Changes in Tick Populations and Subpopulations | Advancements in omics technologies | <repositories>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>http://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>http://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>http://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>http://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>http://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>http://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>http://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>http://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>http://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>http://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>http://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>http://repo.spring.io/release</url>\n    </repository>\n    <repository>\n        <id>spring-milestones</id>\n        <name>Spring Milestones</name>\n        <url>http://repo.spring.io/milestone</url>\n    </repository>\n    <repository>\n        <id>spring-snapshots</id>\n        <name>Spring Snapshots</name>\n        <url>http://repo.spring.io/snapshot</url>\n    </repository>\n    <repository>\n        <id>spring-releases</id>\n        <name>Spring Releases</name>\n        <url>http://repo.spring.io/release</url>\n    </repository>\n    <\n================================================================================\nOutput : \n| Earthquake Magnitude | Date | Location | Impact |\n| --- | --- | --- | --- |\n| 7.8 | Feb. 6, 2020 | Southern Turkey near northern border of Syria | Magnitude 7.8 earthquake followed by magnitude 7.5 earthquake |\n| 7.5 | Feb. 6, 2020 | 59 miles (95 kilometers) to the southwest of Syria | Magnitude 7.5 earthquake was the most devastating to hit earthquake-prone Turkey in more than 20 years and as strong as one in 1939, the most powerful recorded there | <filename>README.md<gh_stars>1-10\n# 2021-01-20\n\n| Date | Title |\n| --- | --- |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021-01-20 |\n| 2021-01-20 | 2021\n================================================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in test_data['output'] :\n    print('Output : ')\n    print(i)\n    print('='*80)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T16:05:27.073766Z","iopub.execute_input":"2024-07-01T16:05:27.074487Z","iopub.status.idle":"2024-07-01T16:05:27.083750Z","shell.execute_reply.started":"2024-07-01T16:05:27.074456Z","shell.execute_reply":"2024-07-01T16:05:27.082660Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Output : \n| Name | Date | Credit Score | Credit Inquiries | Loan | Balance |\n| --- | --- | --- | --- | --- | --- |\n| John Smith | Jan 1,1985 | 720 | Auto Loan (Nov 1, 2020), Credit Card (Feb 15, 2022), Mortgage (Jul 10, 2015) | Credit Cards, Auto Loan, Mortgage, Student Loan | Cards: $3,000, Auto: $5,000, Mortgage: $250,000, Student: $10,000 |\n| Emily Johnson | May 10, 1990 | 680 | Auto Loan, Credit Card, Personal Loan | Credit Cards, Personal Loan, Auto Loan | Cards: $3,500, Collection: $500 |\n| Michael Thompson | N/A | 780 | Auto Loan Refinance, New Credit Card, Debt Consolidation Loan | Credit Cards, Auto Loan, Mortgage, Student Loan | Cards: $2,000, Auto: $10,000, Mortgage: $300,000, Student: $5,000 |\n================================================================================\nOutput : \n| Delivery Date | Route | Vehicle Condition | Departure Time | Arrival Time | Travel Time |\n| --- | --- | --- | --- | --- | --- |\n| June 1, 2023 | Highway 1, Exit 5 to 10 | Intact | 9:00 AM | 11:30 AM | 2h 30m |\n| June 10, 2023 | Interstate 95, Exit 15 to 20 | Intact | 8:30 AM | 2:00 PM | 5h 30m |\n| June 15, 2023 | Highway 3, Exit 8 to 12 | Intact | 10:00 AM | 2:30 PM | 4h 30m |\n================================================================================\nOutput : \n| Clothing Styles | Material | Patterns |\n| --- | --- | --- |\n| Lighter layers | Draped jersey | Distressed |\n| Dresses | Distressed fabrics | Utilitarian |\n| Maxi skirts | Jersey | Minimalism |\n| Tailored vests | Smart tailoring | Smart shirting |\n| Well-pocketed garments | Button-up shirting | N/A |\n================================================================================\nOutput : \n| Species Distribution | Population Abundance |\n| --- | --- |\n| Tropical and subtropical countries | Ticks affect 80% of cattle population worldwide; estimated annual economic loss of USD 20-30 billion |\n| South Africa | Estimated annual economic loss surpassing USD 33 million (ZAR 500 million); Seven major genera of ixodid ticks (Amblyomma, Dermacentor, Haemaphysalis, Hyalomma, Ixodes, and Rhipicephalus) |\n================================================================================\nOutput : \n| Magnitude | Date | Location | Affect |\n| --- | --- | --- | --- |\n| 7.8 | Feb. 6 | Southern Turkey | Earthquake, affected Syrian refugees & aid organizations |\n| 7.5 | Feb. 6 | 59 miles SW of Turkey | Aftershock, state of emergency in 10 provinces |\n================================================================================\n","output_type":"stream"}]}]}